{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-31T00:52:29.600131Z",
     "start_time": "2024-01-31T00:52:19.034509Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/30 19:52:24 WARN Utils: Your hostname, Zachs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.134 instead (on interface en0)\n",
      "24/01/30 19:52:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/01/30 19:52:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/30 19:52:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "path_to_jdbc_driver_jar = \"/Users/pintoza/Desktop/dev/data-science/taxi-demand-forecast/postgresql-42.7.1.jar\"\n",
    "\n",
    "# Increase the memory allocation\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Taxi Data Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.jars\", path_to_jdbc_driver_jar) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|total_amount|DOLocationID|PULocationID|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+\n",
      "|11/14/2018 07:35:...| 11/14/2018 07:41:...|              1|         0.83|         8.8|         230|         142|\n",
      "|11/14/2018 07:35:...| 11/14/2018 08:00:...|              1|          4.1|       24.13|          65|         137|\n",
      "|11/14/2018 07:35:...| 11/14/2018 07:38:...|              1|          0.4|         6.8|         234|         164|\n",
      "|11/14/2018 07:35:...| 11/14/2018 07:44:...|              1|         1.56|        10.8|         236|         237|\n",
      "|11/14/2018 07:35:...| 11/14/2018 07:49:...|              1|         2.01|        13.3|         262|         237|\n",
      "|11/14/2018 07:35:...| 11/14/2018 08:27:...|              1|         8.05|        38.8|          66|         163|\n",
      "|11/14/2018 07:35:...| 11/14/2018 07:51:...|              1|          3.2|        19.1|         151|         237|\n",
      "|11/14/2018 07:35:...| 11/14/2018 07:42:...|              2|          0.4|        9.35|         237|         237|\n",
      "|11/14/2018 07:35:...| 11/14/2018 08:01:...|              1|         2.63|       21.59|         142|         233|\n",
      "|11/14/2018 07:35:...| 11/14/2018 07:40:...|              1|          0.9|        8.75|         249|         211|\n",
      "|11/14/2018 07:35:...| 11/14/2018 07:55:...|              1|         2.59|        15.8|         164|         141|\n",
      "|11/14/2018 07:35:...| 11/14/2018 07:53:...|              2|          2.4|       18.35|          79|         161|\n",
      "|11/14/2018 07:35:...| 11/14/2018 07:47:...|              1|         2.66|       14.76|         181|          66|\n",
      "|11/14/2018 07:35:...| 11/14/2018 07:51:...|              4|         0.98|        12.8|         239|         142|\n",
      "|11/14/2018 07:35:...| 11/14/2018 07:58:...|              1|         2.34|        18.8|         113|          68|\n",
      "|11/14/2018 07:35:...| 11/14/2018 07:56:...|              1|          2.3|       18.05|         249|         230|\n",
      "|11/14/2018 07:35:...| 11/14/2018 07:55:...|              1|          3.2|        19.5|         166|         236|\n",
      "|11/14/2018 07:35:...| 11/14/2018 07:39:...|              1|         0.65|         6.8|         229|         140|\n",
      "|11/14/2018 07:35:...| 11/14/2018 07:59:...|              5|         3.93|       24.36|         166|          48|\n",
      "|11/14/2018 07:35:...| 11/14/2018 08:03:...|              2|         4.84|        24.3|         261|          48|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve variables\n",
    "database_url = os.getenv(\"DB_URL\")\n",
    "db_user = os.getenv(\"DB_USERNAME\")\n",
    "db_password = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "# Database connection properties\n",
    "properties = {\"user\": db_user, \"password\": db_password, \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "# Test query to load a small subset of data\n",
    "query = \"(SELECT * FROM taxi_trips LIMIT 100) AS subquery\"\n",
    "df = spark.read.jdbc(url=database_url, table=query, properties=properties)\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T00:52:57.855053Z",
     "start_time": "2024-01-31T00:52:47.057607Z"
    }
   },
   "id": "4f295b9e34904d8",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, max as max_\n",
    "\n",
    "# Calculate the mode of passenger_count\n",
    "mode_value = df.groupBy(\"passenger_count\").count().orderBy(col(\"count\").desc()).first()[\"passenger_count\"]\n",
    "mode_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T00:59:05.199483Z",
     "start_time": "2024-01-31T00:59:03.227497Z"
    }
   },
   "id": "c9890f4ed5793c08",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, lit\n",
    "\n",
    "# Impute NA values with the mode\n",
    "df = df.withColumn(\"passenger_count\", coalesce(df[\"passenger_count\"], lit(mode_value)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T01:00:10.217558Z",
     "start_time": "2024-01-31T01:00:10.138146Z"
    }
   },
   "id": "f2c33165f3842e03",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|total_amount|DOLocationID|PULocationID|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+\n",
      "|                   0|                    0|              0|            0|           0|           0|           0|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when, isnull\n",
    "\n",
    "# Counting the number of nulls in each column\n",
    "null_counts = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns])\n",
    "\n",
    "# Show the count of nulls for each column\n",
    "null_counts.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T01:06:49.603108Z",
     "start_time": "2024-01-31T01:06:48.700594Z"
    }
   },
   "id": "9a4e30dae91dffe5",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: long (nullable = false)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "# Print the schema to check the data types\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T01:14:38.314478Z",
     "start_time": "2024-01-31T01:14:38.279493Z"
    }
   },
   "id": "820717dd35f8b261",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: long (nullable = false)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "# Convert datetime fields to TimestampType\n",
    "df = df.withColumn(\"tpep_pickup_datetime\", to_timestamp(df[\"tpep_pickup_datetime\"], 'MM/dd/yyyy hh:mm:ss a')) \\\n",
    "       .withColumn(\"tpep_dropoff_datetime\", to_timestamp(df[\"tpep_dropoff_datetime\"], 'MM/dd/yyyy hh:mm:ss a'))\n",
    "\n",
    "# Show the schema again to verify the changes\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T01:14:50.031691Z",
     "start_time": "2024-01-31T01:14:49.902337Z"
    }
   },
   "id": "99bc79135ca85e34",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to a Parquet file in the /data/processed/ directory\n",
    "# Modify the path as necessary based on the above suggestions\n",
    "df.write.parquet(\"file:///Users/pintoza/Desktop/dev/data-science/taxi-demand-forecast/data/processed/taxi_data_processed\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T01:30:15.531317Z",
     "start_time": "2024-01-31T01:30:12.995499Z"
    }
   },
   "id": "bfd5d88e0144696c",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "697172abdd28778f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
